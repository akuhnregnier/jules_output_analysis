{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import cf_units\n",
    "import iris\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from alepython import ale_plot\n",
    "from joblib import Memory\n",
    "from numba import jit, njit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from wildfires.analysis import corr_plot, cube_plotting\n",
    "from wildfires.data import MonthlyDataset, homogenise_cube_attributes\n",
    "from wildfires.utils import ensure_datetime\n",
    "\n",
    "filterwarnings(\"ignore\", \".*divide by zero.*\")\n",
    "memory = Memory(\".cache\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation test splitting.\n",
    "train_test_split_kwargs = dict(random_state=1, shuffle=True, test_size=0.3)\n",
    "\n",
    "# Specify common RF (training) params.\n",
    "n_splits = 5\n",
    "\n",
    "default_param_dict = {\"random_state\": 1, \"bootstrap\": True}\n",
    "\n",
    "# XXX\n",
    "param_dict = {\n",
    "    **default_param_dict,\n",
    "    \"ccp_alpha\": 2e-9,\n",
    "    \"max_depth\": 15,\n",
    "    \"max_features\": \"auto\",\n",
    "    \"min_samples_leaf\": 4,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"n_estimators\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mm_indices(master_mask):\n",
    "    mm_valid_indices = np.where(~master_mask.ravel())[0]\n",
    "    mm_valid_train_indices, mm_valid_val_indices = train_test_split(\n",
    "        mm_valid_indices,\n",
    "        **train_test_split_kwargs,\n",
    "    )\n",
    "    return mm_valid_indices, mm_valid_train_indices, mm_valid_val_indices\n",
    "\n",
    "\n",
    "def get_mm_data(x, master_mask, kind):\n",
    "    \"\"\"Return masked master_mask copy and training or validation indices.\n",
    "\n",
    "    The master_mask copy is filled using the given data.\n",
    "\n",
    "    Args:\n",
    "        x (array-like): Data to use.\n",
    "        master_mask (array):\n",
    "        kind ({'train', 'val'})\n",
    "\n",
    "    Returns:\n",
    "        masked_data, mm_indices:\n",
    "\n",
    "    \"\"\"\n",
    "    mm_valid_indices, mm_valid_train_indices, mm_valid_val_indices = get_mm_indices(\n",
    "        master_mask\n",
    "    )\n",
    "    masked_data = np.ma.MaskedArray(\n",
    "        np.zeros_like(master_mask, dtype=np.float64), mask=np.ones_like(master_mask)\n",
    "    )\n",
    "    if kind == \"train\":\n",
    "        masked_data.ravel()[mm_valid_train_indices] = x\n",
    "    elif kind == \"val\":\n",
    "        masked_data.ravel()[mm_valid_val_indices] = x\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kind: {kind}\")\n",
    "    return masked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def isclose(a, b, atol=1e-4):\n",
    "    return np.abs(a - b) < atol\n",
    "\n",
    "\n",
    "assert isclose(1, 1)\n",
    "assert isclose(1, 1 + 1e-5)\n",
    "assert not isclose(1, 1 + 1e-5, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def find_gridpoint(land_lat, land_lon, grid_lats, grid_lons):\n",
    "    \"\"\"Mapping from a single land coordinate to the matching grid indices.\"\"\"\n",
    "    for lat_i, grid_lat in enumerate(grid_lats):\n",
    "        for lon_i, grid_lon in enumerate(grid_lons):\n",
    "            if isclose(land_lat, grid_lat) and isclose(land_lon, grid_lon):\n",
    "                return lat_i, lon_i\n",
    "    raise RuntimeError(\"Matching gridpoint not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(find_gridpoint(0, 0, np.array([-1, 0, 1]), np.array([-1, 0, 1])))\n",
    "print(\n",
    "    find_gridpoint(\n",
    "        0.5, 0.5, np.array([-1, 0, 0.5, 1]), np.array([-1, 0, 0.25, 0.5, 0.75, 1])\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    find_gridpoint(\n",
    "        0,\n",
    "        0,\n",
    "        np.linspace(-90, 90, 100, endpoint=False),\n",
    "        np.linspace(-90, 90, 100, endpoint=False),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def get_grid_mask(mask, orig_lats, orig_lons, grid_lats, grid_lons):\n",
    "    \"\"\"Calculate mask to transition from one grid to another.\n",
    "\n",
    "    Note:\n",
    "        This probably relies on the contiguity structure of the arrays.\n",
    "\n",
    "    \"\"\"\n",
    "    # XXX: Would it not be simpler and almost equally robust to look at the spacing between coordinates and use this to infer the index? i.e. np.rint(lats - lats[0] / (lats[1] - lats[0])), as in jules.py?\n",
    "    for (land_i, (land_lat, land_lon)) in enumerate(\n",
    "        zip(tqdm(orig_lats, desc=\"Land gridpoint\"), orig_lons)\n",
    "    ):\n",
    "        lat_i, lon_i = find_gridpoint(land_lat, land_lon, grid_lats, grid_lons)\n",
    "        mask[..., lat_i, lon_i] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube_1d_to_2d(cube):\n",
    "    \"\"\"Convert JULES output on 1D grid to 2D grid.\"\"\"\n",
    "    land_grid_coord = -1  # The last axis is associated with the spatial domain.\n",
    "\n",
    "    assert land_grid_coord == -1\n",
    "\n",
    "    lat_coord = cube.coord(\"latitude\")\n",
    "    lon_coord = cube.coord(\"longitude\")\n",
    "\n",
    "    orig_lats = lat_coord.points.data.ravel()\n",
    "    orig_lons = lon_coord.points.data.ravel()\n",
    "\n",
    "    lat_step = np.unique(np.diff(np.sort(orig_lats)))[1]\n",
    "    lon_step = np.unique(np.diff(np.sort(orig_lons)))[1]\n",
    "\n",
    "    # Use the latitude and longitude steps from above to determine the number of\n",
    "    # latitude and longitude steps.\n",
    "    n_lat = round(180 / lat_step)\n",
    "    n_lon = round(360 / lon_step)\n",
    "\n",
    "    # Ensure that these represent a regular grid.\n",
    "    assert np.isclose(n_lat * lat_step, 180)\n",
    "    assert np.isclose(n_lon * lon_step, 360)\n",
    "\n",
    "    # Create a grid of ..., lat, lon points to match the shape of the given cube.\n",
    "    new_shape = tuple(list(cube.shape[:land_grid_coord]) + [n_lat, n_lon])\n",
    "\n",
    "    # Now convert the 1D data to the 2D array created above, using a mask.\n",
    "    mask = np.zeros((n_lat, n_lon), dtype=np.bool_)\n",
    "\n",
    "    grid_lats = np.linspace(-90, 90, n_lat, endpoint=False)\n",
    "    grid_lons = np.linspace(0, 360, n_lon, endpoint=False)\n",
    "\n",
    "    mask = get_grid_mask(mask, orig_lats, orig_lons, grid_lats, grid_lons)\n",
    "\n",
    "    if len(np.squeeze(cube.data).shape) == 1:\n",
    "        # Simply assign based on the mask.\n",
    "        new_data = np.ma.MaskedArray(np.zeros_like(mask, dtype=np.float64), mask=True)\n",
    "        new_data[mask] = np.squeeze(cube.data)\n",
    "    elif len(np.squeeze(cube.data).shape) > 1:\n",
    "        # Iterate over earlier dimensions.\n",
    "        new_data = np.ma.MaskedArray(np.zeros(new_shape, dtype=np.float64), mask=True)\n",
    "        for indices in product(*(range(l) for l in cube.shape[:-1])):\n",
    "            sel = (*indices, slice(None))\n",
    "            new_data[sel][mask] = cube.data[sel]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid cube shape {cube.shape}\")\n",
    "\n",
    "    # XXX: Assumes more than 1 lat, lon coord, and destroys other dimensions.\n",
    "    new_data = np.squeeze(new_data)\n",
    "\n",
    "    n_dim = len(new_data.shape)\n",
    "    lat_dim = n_dim - 2\n",
    "    lon_dim = n_dim - 1\n",
    "\n",
    "    new_cube = iris.cube.Cube(\n",
    "        new_data,\n",
    "        dim_coords_and_dims=[\n",
    "            (\n",
    "                iris.coords.DimCoord(\n",
    "                    grid_lats, standard_name=\"latitude\", units=\"degrees\"\n",
    "                ),\n",
    "                lat_dim,\n",
    "            ),\n",
    "            (\n",
    "                iris.coords.DimCoord(\n",
    "                    grid_lons, standard_name=\"longitude\", units=\"degrees\"\n",
    "                ),\n",
    "                lon_dim,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    new_cube.metadata = cube.metadata\n",
    "    return new_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(\".cache\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jules_gws_dir = Path(\"/gws/nopw/j04/jules\")\n",
    "source_dir = jules_gws_dir / \"stephanemangeon/FireMIP_fixed_clim\"\n",
    "assert source_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"FireMIP.inferno.fixed_clim.Monthly.2013.nc\"  # Contains aggregated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes = iris.load(str(source_dir / data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cube in tqdm(cubes, desc=\"Plotting cubes (single timeslice)\"):\n",
    "    cube_2d = cube_1d_to_2d(cube[0])\n",
    "\n",
    "    assert len(cube_2d.shape) >= 2\n",
    "\n",
    "    if len(cube_2d.shape) == 2:\n",
    "        sel = slice(None)\n",
    "    else:\n",
    "        for indices in product(*(range(l) for l in cube_2d.shape[:-2])):\n",
    "            sel = (*indices, slice(None), slice(None))\n",
    "    try:\n",
    "        fig = cube_plotting(cube_2d[sel])\n",
    "    except Exception as e:\n",
    "        print(\"cube:\", str(cube))\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_cube = cubes.extract_strict(iris.Constraint(name=\"PFT burnt area fraction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_2d = cube_1d_to_2d(ba_cube[10][0])\n",
    "ba_2d.data.mask |= np.isnan(ba_2d.data)\n",
    "fig = cube_plotting(ba_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted({cube.long_name for cube in cubes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_names = [\n",
    "    \"Gridbox precipitation rate\",\n",
    "    \"Gridbox soil carbon (total)\",\n",
    "    \"Gridbox soil carbon in each pool (DPM,RPM,bio,hum)\",\n",
    "    \"Gridbox surface evapotranspiration from soil moisture store\",\n",
    "    \"Gridbox surface temperature\",\n",
    "    \"Gridbox unfrozen moisture content of each soil layer as a fraction of saturation\",\n",
    "    \"PFT burnt area fraction\",\n",
    "    \"PFT gross primary productivity\",\n",
    "    \"PFT leaf area index\",\n",
    "    \"PFT net primary productivity\",\n",
    "    \"PFT soil moisture availability factor (beta)\",\n",
    "    \"PFT total carbon content of the vegetation at the end of model timestep.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_dict = {\n",
    "    \"UK\": (51.5, 0),\n",
    "    \"Uganda\": (2.36, 32.51),\n",
    "    \"Durban\": (-29.13, 31),\n",
    "    \"Cape Town\": (-33.57, 19.28),\n",
    "    \"Yosemite\": (37.7, 360 - 119.67),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_months = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_carbon_pools = (\"DPM\", \"RPM\", \"bio\", \"hum\")\n",
    "pfts = list(range(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with individual PFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (location, (lat, lon)) in lat_lon_dict.items():\n",
    "    fig, axes = plt.subplots(\n",
    "        len(cube_names),\n",
    "        1,\n",
    "        figsize=(10, 3.5 * len(cube_names)),\n",
    "        sharex=True,\n",
    "        constrained_layout=True,\n",
    "        dpi=120,\n",
    "    )\n",
    "    fig.suptitle(location)\n",
    "\n",
    "    for ax, var in zip(axes, cube_names):\n",
    "        ext_cube = cubes.extract_strict(iris.Constraint(name=var))\n",
    "        var_cube = cube_1d_to_2d(ext_cube)[-n_months:]\n",
    "        var_cube.add_dim_coord(ext_cube.coord(\"time\")[-n_months:], 0)\n",
    "\n",
    "        print(var_cube.long_name, var_cube.shape)\n",
    "\n",
    "        ax.set_title(f\"{var_cube.long_name} ({var_cube.units})\")\n",
    "\n",
    "        lat_i = np.argmin(np.abs(var_cube.coord(\"latitude\").points - lat))\n",
    "        lat_sel = var_cube.coord(\"latitude\").points[lat_i]\n",
    "\n",
    "        lon_i = np.argmin(np.abs(var_cube.coord(\"longitude\").points - lon))\n",
    "        lon_sel = var_cube.coord(\"longitude\").points[lon_i]\n",
    "\n",
    "        if len(var_cube.shape) == 3:\n",
    "            plot_data_list = [var_cube[:, lat_i, lon_i].data.copy()]\n",
    "        elif len(var_cube.shape) == 4:\n",
    "            plot_data_list = [\n",
    "                var_cube[:, i, lat_i, lon_i].data.copy()\n",
    "                for i in range(var_cube.shape[1])\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        for i, plot_data in enumerate(plot_data_list):\n",
    "            plot_data_mean = np.mean(plot_data)\n",
    "            plot_data -= plot_data_mean\n",
    "\n",
    "            if \"soil carbon\" in var_cube.long_name and \"pool\" in var_cube.long_name:\n",
    "                label = soil_carbon_pools[i]\n",
    "            else:\n",
    "                label = str(i)\n",
    "\n",
    "            ax.plot(\n",
    "                [\n",
    "                    ensure_datetime(var_cube.coord(\"time\").cell(i).point)\n",
    "                    for i in range(var_cube.shape[0])\n",
    "                ],\n",
    "                plot_data,\n",
    "                label=label,\n",
    "                marker=\".\",\n",
    "            )\n",
    "        ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with mean over individual PFTs and pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    len(cube_names),\n",
    "    1,\n",
    "    figsize=(9, 3 * len(cube_names)),\n",
    "    sharex=True,\n",
    "    constrained_layout=True,\n",
    "    dpi=120,\n",
    ")\n",
    "\n",
    "for ax, var in zip(axes, cube_names):\n",
    "    ext_cube = cubes.extract_strict(iris.Constraint(name=var))\n",
    "    if \"generic\" in list(coord.name() for coord in ext_cube.coords()):\n",
    "        # Average over carbon pools or PFTs\n",
    "        ext_cube = ext_cube.collapsed(\"generic\", iris.analysis.MEAN)\n",
    "\n",
    "    var_cube = cube_1d_to_2d(ext_cube)[-n_months:]\n",
    "    var_cube.add_dim_coord(ext_cube.coord(\"time\")[-n_months:], 0)\n",
    "\n",
    "    print(var_cube.long_name, var_cube.shape)\n",
    "\n",
    "    ax.set_title(f\"{var_cube.long_name} ({var_cube.units})\")\n",
    "\n",
    "    for (location, (lat, lon)) in lat_lon_dict.items():\n",
    "        lat_i = np.argmin(np.abs(var_cube.coord(\"latitude\").points - lat))\n",
    "        lat_sel = var_cube.coord(\"latitude\").points[lat_i]\n",
    "\n",
    "        lon_i = np.argmin(np.abs(var_cube.coord(\"longitude\").points - lon))\n",
    "        lon_sel = var_cube.coord(\"longitude\").points[lon_i]\n",
    "\n",
    "        assert len(var_cube.shape) == 3\n",
    "\n",
    "        plot_data = var_cube[:, lat_i, lon_i].data.copy()\n",
    "\n",
    "        plot_data_mean = np.mean(plot_data)\n",
    "        plot_data -= plot_data_mean\n",
    "\n",
    "        ax.plot(\n",
    "            [\n",
    "                ensure_datetime(var_cube.coord(\"time\").cell(i).point)\n",
    "                for i in range(var_cube.shape[0])\n",
    "            ],\n",
    "            plot_data,\n",
    "            label=location,\n",
    "            marker=\".\",\n",
    "        )\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_months = 49\n",
    "\n",
    "ref_unit = cf_units.Unit(\"seconds since 1900-1-1\", calendar=\"365_day\")\n",
    "\n",
    "processed_cubes = iris.cube.CubeList([])\n",
    "for var in tqdm(cube_names, desc=\"Processing cubes\"):\n",
    "    ext_cube = cubes.extract_strict(iris.Constraint(name=var))\n",
    "    if \"generic\" in list(coord.name() for coord in ext_cube.coords()):\n",
    "        # Average over carbon pools or PFTs\n",
    "        ext_cube = ext_cube.collapsed(\"generic\", iris.analysis.MEAN)\n",
    "\n",
    "    var_cube = cube_1d_to_2d(ext_cube)[-proc_months:-1]\n",
    "\n",
    "    time_coord = ext_cube.coord(\"time\")[-proc_months:-1]\n",
    "    assert time_coord.units == ref_unit\n",
    "    time_coord.bounds = None\n",
    "    # Sometimes ~5 minutes may be missing to get to the next day.\n",
    "    time_coord.points = time_coord.points + 5 * 60\n",
    "\n",
    "    var_cube.add_dim_coord(time_coord, 0)\n",
    "\n",
    "    processed_cubes.append(var_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_insts = []\n",
    "\n",
    "for proc_cube in tqdm(processed_cubes):\n",
    "    # Create a new Dataset for each cube.\n",
    "    proc_inst = type(\n",
    "        proc_cube.long_name.replace(\" \", \"\"),\n",
    "        (MonthlyDataset,),\n",
    "        {\n",
    "            \"__init__\": lambda self: None,\n",
    "            \"frequency\": \"monthly\",\n",
    "        },\n",
    "    )()\n",
    "    proc_inst.cubes = iris.cube.CubeList([proc_cube])\n",
    "\n",
    "    proc_insts.append(\n",
    "        proc_inst.get_climatology_dataset(proc_inst.min_time, proc_inst.max_time)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_proc_cubes = []\n",
    "\n",
    "ba_cube = None\n",
    "\n",
    "for proc_inst in proc_insts:\n",
    "    if \"burnt area\" in proc_inst.cube.long_name:\n",
    "        ba_cube = proc_inst.cube\n",
    "        continue\n",
    "    shifted_proc_cubes.append(proc_inst.cube)\n",
    "    for shift in (1, 3, 6, 9):\n",
    "        # XXX: Overly simplistic np.roll() implementation!!\n",
    "        c2 = proc_inst.cube.copy()\n",
    "        c2.data = np.roll(proc_inst.cube.data, shift, axis=0)\n",
    "        c2.long_name = c2.long_name + f\"({shift})\"\n",
    "        c2.var_name = None\n",
    "        c2.short_name = None\n",
    "\n",
    "        shifted_proc_cubes.append(c2)\n",
    "\n",
    "assert ba_cube is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3), dpi=150)\n",
    "for i in np.array([0, 1, 2]) + 35:\n",
    "    print(cube.coord(\"month_number\").points)\n",
    "    cube = shifted_proc_cubes[i]\n",
    "    plt.plot(\n",
    "        cube.coord(\"month_number\").points,\n",
    "        cube.data[:, 72, 110],\n",
    "        label=cube.name(),\n",
    "        alpha=0.9,\n",
    "        marker=\"x\",\n",
    "    )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_mask = ba_cube.data.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog_data = pd.Series(ba_cube.data.data[~master_mask])\n",
    "endog_data.name = \"burnt area\"\n",
    "\n",
    "exog_dict = {}\n",
    "for cube in shifted_proc_cubes:\n",
    "    exog_dict[cube.long_name] = cube.data.data[~master_mask]\n",
    "\n",
    "exog_data = pd.DataFrame(exog_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten_mapping = {\n",
    "    \"Gridbox\": \"\",\n",
    "    \"precipitation\": \"precip\",\n",
    "    \"soil carbon in each pool\": \"soil pool carbon\",\n",
    "    \"surface\": \"surf\",\n",
    "    \"evapotranspiration\": \"evapot\",\n",
    "    \"from soil moisture store\": \"soil moist\",\n",
    "    \"temperature\": \"temp\",\n",
    "    \"unfrozen moisture content of each soil layer as a fraction of saturation\": \"unfrozen moist soil layer / sat\",\n",
    "    \"gross primary productivity\": \"gpp\",\n",
    "    \"net primary productivity\": \"npp\",\n",
    "    \"soil moisture availability factor (beta)\": \"soil moist avail fact\",\n",
    "    \"total carbon content of the vegetation at the end of model timestep\": \"total veg C end timestep\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_columns(df):\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        for old, new in shorten_mapping.items():\n",
    "            col = col.replace(old, new)\n",
    "        col = col.strip()\n",
    "        new_cols.append(col)\n",
    "    df.columns = new_cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten_columns(exog_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the BA data so it has higher magnitudes\n",
    "\n",
    "This seems to be required by the RF algorithm in order to make any predictions at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog_data /= np.max(endog_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    exog_data, endog_data, **train_test_split_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(**param_dict)\n",
    "model.n_jobs = 3\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train:\", r2_score(y_train_pred, y_train))\n",
    "print(\"val:\", r2_score(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test.values)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cube_plotting(\n",
    "    get_mm_data(y_test.values, master_mask, \"val\"),\n",
    "    boundaries=[0, 1e-3, 1e-2, 0.05, 0.2, 0.5],\n",
    "    cmap=\"inferno\",\n",
    "    fig=plt.figure(figsize=(7, 3.2), dpi=150),\n",
    "    colorbar_kwargs=dict(label=\"burnt area (scaled)\"),\n",
    "    title=\"Validation Observations\",\n",
    "    extend='neither',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cube_plotting(\n",
    "    get_mm_data(y_test_pred, master_mask, \"val\"),\n",
    "    boundaries=[0, 1e-3, 1e-2, 0.05, 0.2, 0.5],\n",
    "    cmap=\"inferno\",\n",
    "    fig=plt.figure(figsize=(7, 3.2), dpi=150),\n",
    "    colorbar_kwargs=dict(label=\"burnt area (scaled)\"),\n",
    "    title=\"Validation Predictions\",\n",
    "    extend='neither',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250)\n",
    "plt.hexbin(exog_data[\"surf temp\"], endog_data, bins=\"log\")\n",
    "plt.xlabel('surface temperature')\n",
    "_ = plt.ylabel('BA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250)\n",
    "plt.hexbin(y_train, y_train_pred, bins=\"log\")\n",
    "plt.xlabel(\"observed BA (train)\")\n",
    "_ = plt.ylabel(\"predicted BA (train)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250)\n",
    "plt.hexbin(y_test, y_test_pred, bins=\"log\")\n",
    "plt.xlabel(\"observed BA (test)\")\n",
    "_ = plt.ylabel(\"predicted BA (test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_trees_gini = pd.DataFrame(\n",
    "    [tree.feature_importances_ for tree in model],\n",
    "    columns=X_train.columns,\n",
    ")\n",
    "mean_importances = ind_trees_gini.mean().sort_values(ascending=False)\n",
    "ind_trees_gini = ind_trees_gini.reindex(mean_importances.index, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4), dpi=170)\n",
    "\n",
    "N_col = 18\n",
    "\n",
    "sns.boxplot(data=ind_trees_gini.iloc[:, :N_col], ax=ax)\n",
    "ax.set(\n",
    "    # title=\"Gini Importances\",\n",
    "    ylabel=\"Gini Importance\\n\"\n",
    ")\n",
    "_ = plt.setp(ax.xaxis.get_majorticklabels(), rotation=60, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_trees_gini = pd.DataFrame(\n",
    "    [tree.feature_importances_ for tree in model],\n",
    "    columns=X_train.columns,\n",
    ")\n",
    "mean_importances = ind_trees_gini.mean().sort_values(ascending=False)\n",
    "ind_trees_gini = ind_trees_gini.reindex(mean_importances.index, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4), dpi=170)\n",
    "\n",
    "N_col = 30\n",
    "\n",
    "sns.boxplot(data=ind_trees_gini.iloc[:, :N_col], ax=ax)\n",
    "ax.set(\n",
    "    # title=\"Gini Importances\",\n",
    "    ylabel=\"Gini Importance\\n\"\n",
    ")\n",
    "_ = plt.setp(ax.xaxis.get_majorticklabels(), rotation=60, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in tqdm(X_train.columns, desc=\"1D ALE plots\"):\n",
    "    fig, axes = ale_plot(\n",
    "        model,\n",
    "        X_train,\n",
    "        feature,\n",
    "        bins=20,\n",
    "        fig=plt.figure(figsize=(5.5, 2), dpi=150),\n",
    "        quantile_axis=True,\n",
    "        monte_carlo=True,\n",
    "        monte_carlo_rep=10,\n",
    "        monte_carlo_ratio=0.2,\n",
    "    )\n",
    "    plt.setp(axes[\"ale\"].xaxis.get_majorticklabels(), rotation=60, ha=\"right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wildfires] *",
   "language": "python",
   "name": "conda-env-wildfires-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
