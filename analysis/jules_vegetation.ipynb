{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import iris\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML, display\n",
    "from iris.coord_categorisation import add_month_number\n",
    "from tqdm.auto import tqdm\n",
    "from wildfires.analysis import cube_plotting\n",
    "from wildfires.configuration import DATA_DIR\n",
    "from wildfires.data import regions_GFED\n",
    "from wildfires.utils import match_shape\n",
    "\n",
    "from jules_output_analysis.data import (\n",
    "    cube_1d_to_2d,\n",
    "    dummy_lat_lon_cube,\n",
    "    frac_weighted_mean,\n",
    "    get_climatology_cube,\n",
    "    get_n96e_land_mask,\n",
    "    load_lat_lon_coords,\n",
    "    regrid_to_n96e,\n",
    ")\n",
    "\n",
    "filterwarnings(\"ignore\", \".*divide by zero.*\")\n",
    "filterwarnings(\"ignore\", \".*invalid units.*\")\n",
    "filterwarnings(\"ignore\", \".*may not be fully.*\")\n",
    "filterwarnings(\"ignore\", \".*axes.*\")\n",
    "filterwarnings(\"ignore\")\n",
    "mpl.rc_file(\"matplotlibrc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = str(Path(\"~/tmp/climatology5_c.nc\").expanduser())\n",
    "lat_coord, lon_coord = load_lat_lon_coords(source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c.name() for c in iris.load_raw(source_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c.name() for c in iris.load_raw(source_file) if \"prod\" in c.name()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    \"pft_lai\": dict(\n",
    "        var_name=\"PFT leaf area index\",\n",
    "        label=\"LAI (1)\",\n",
    "        name=\"JULES LAI\",\n",
    "    ),\n",
    "    \"pft_fapar\": dict(\n",
    "        var_name=\"PFT Fraction of Absorbed Photosynthetically Active Radiation\",\n",
    "        label=\"FAPAR (1)\",\n",
    "        name=\"JULES FAPAR\",\n",
    "    ),\n",
    "    \"frac\": dict(\n",
    "        var_name=\"Fractional cover of each surface type\",\n",
    "        label=\"1\",\n",
    "        name=\"JULES Frac\",\n",
    "    ),\n",
    "    #     'npp': dict(\n",
    "    #         var_name=\"Gridbox net primary productivity prior to N limitation\",\n",
    "    #     ),\n",
    "    #     'gpp': dict(\n",
    "    #         var_name=\"Gridbox gross primary productivity\",\n",
    "    #     ),\n",
    "    \"pft_gpp\": dict(\n",
    "        var_name=\"PFT gross primary productivity\",\n",
    "        label=\"GPP\",\n",
    "        name=\"JULES GPP\",\n",
    "    ),\n",
    "    \"pft_npp\": dict(\n",
    "        var_name=\"PFT net primary productivity prior to N limitation\",\n",
    "        label=\"NPP\",\n",
    "        name=\"JULES NPP\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "for name, data in variables.items():\n",
    "    data[\"raw_cube\"] = iris.load_cube(source_file, constraint=data[\"var_name\"])\n",
    "    data[\"avg_cube\"] = data[\"raw_cube\"].collapsed(\"time\", iris.analysis.MEAN)\n",
    "    add_month_number(data[\"raw_cube\"], \"time\")\n",
    "    data[\"mon_avg_cube\"] = data[\"raw_cube\"].aggregated_by(\n",
    "        \"month_number\", iris.analysis.MEAN\n",
    "    )\n",
    "\n",
    "    # Check that the points are correctly ordered.\n",
    "    month_number_points = data[\"mon_avg_cube\"].coord(\"month_number\").points\n",
    "    assert np.all(np.sort(month_number_points) == month_number_points)\n",
    "\n",
    "    # Promote the AuxCoord to a DimCoord.\n",
    "    data[\"mon_avg_cube\"].replace_coord(\n",
    "        iris.coords.DimCoord.from_coord(data[\"mon_avg_cube\"].coord(\"month_number\"))\n",
    "    )\n",
    "    # Remove the time coordinate.\n",
    "    data[\"mon_avg_cube\"].remove_coord(\"time\")\n",
    "    assert len(data[\"mon_avg_cube\"].coords(dimensions=0)) == 1\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    for avg_name, check_n in ((\"avg_cube\", 2), (\"mon_avg_cube\", 3)):\n",
    "        N = len(data[avg_name].shape)\n",
    "        assert N >= check_n\n",
    "\n",
    "        if not data[avg_name].coords(\"latitude\"):\n",
    "            data[avg_name].add_aux_coord(lat_coord, (N - 2, N - 1))\n",
    "        if not data[avg_name].coords(\"longitude\"):\n",
    "            data[avg_name].add_aux_coord(lon_coord, (N - 2, N - 1))\n",
    "\n",
    "        data[f\"{avg_name}_2d\"] = cube_1d_to_2d(data[avg_name])\n",
    "\n",
    "        print(data[avg_name].shape, data[f\"{avg_name}_2d\"].shape)\n",
    "\n",
    "\n",
    "def frac_weighted_mean(cube_2d):\n",
    "    avg_frac_2d = variables[\"frac\"][\"avg_cube_2d\"]\n",
    "    assert cube_2d.shape[-3] == 13, cube_2d.shape\n",
    "    assert avg_frac_2d.shape[0] == 17\n",
    "    return np.sum(avg_frac_2d[..., :13, :, :].data * cube_2d.data, axis=-3) / np.sum(\n",
    "        avg_frac_2d[..., :13, :, :].data, axis=-3\n",
    "    )\n",
    "\n",
    "\n",
    "for name, data in variables.items():\n",
    "    if name == \"frac\":\n",
    "        continue\n",
    "\n",
    "    print(f\"Taking frac weighted average of '{name}'.\")\n",
    "    data[\"w_frac_avg_cube_2d\"] = frac_weighted_mean(data[\"avg_cube_2d\"])\n",
    "    data[\"w_frac_mon_avg_cube_2d\"] = frac_weighted_mean(data[\"mon_avg_cube_2d\"])\n",
    "    print(data[\"w_frac_avg_cube_2d\"].shape, data[\"w_frac_mon_avg_cube_2d\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in variables.items():\n",
    "    if name == \"frac\":\n",
    "        continue\n",
    "\n",
    "    cube_plotting(\n",
    "        data.get(\"w_frac_avg_cube_2d\", data[\"avg_cube_2d\"]),\n",
    "        title=name,\n",
    "        colorbar_kwargs=dict(label=name),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAPAR and Antecedent FAPAR (fuel build up) for certain pixels (small regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat = 0\n",
    "# lon = 15\n",
    "# constraint = iris.Constraint(\n",
    "#     latitude=lambda c: lat < c.point < lat + 5,\n",
    "#     longitude=lambda c: lon < c.point < lon + 5,\n",
    "# )\n",
    "\n",
    "# plt.plot(\n",
    "#     dummy_lat_lon_cube(fapar_2d)\n",
    "#     .extract(constraint)\n",
    "#     .collapsed((\"latitude\", \"longitude\"), iris.analysis.MEAN)\n",
    "#     .data,\n",
    "#     label=\"fapar\",\n",
    "# )\n",
    "# plt.plot(\n",
    "#     dummy_lat_lon_cube(fuel_build_up_2d)\n",
    "#     .extract(constraint)\n",
    "#     .collapsed((\"latitude\", \"longitude\"), iris.analysis.MEAN)\n",
    "#     .data,\n",
    "#     label=\"fuel build up\",\n",
    "# )\n",
    "# plt.legend()\n",
    "# _ = plt.title(f\"lat: {lat}, lon: {lon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load observed, reference LAI and FAPAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Obs. LAI\"\n",
    "\n",
    "ref_lai_cube_2d = regrid_to_n96e(\n",
    "    iris.load_cube(str(Path(DATA_DIR) / \"LAI_climatology.nc\"))\n",
    ")\n",
    "ref_lai_cube_2d.data.mask |= match_shape(\n",
    "    ~get_n96e_land_mask(),\n",
    "    ref_lai_cube_2d.shape,\n",
    ")\n",
    "\n",
    "display(HTML(ref_lai_cube_2d._repr_html_()))\n",
    "\n",
    "ref_avg_lai_cube_2d = ref_lai_cube_2d.collapsed(\"time\", iris.analysis.MEAN)\n",
    "ref_max_lai_cube_2d = ref_lai_cube_2d.collapsed(\"time\", iris.analysis.MAX)\n",
    "ref_std_lai_cube_2d = ref_avg_lai_cube_2d.copy(\n",
    "    data=np.std(ref_lai_cube_2d.data, axis=0)\n",
    ")\n",
    "\n",
    "fig = cube_plotting(ref_avg_lai_cube_2d, title=f\"{target} mean\")\n",
    "fig = cube_plotting(ref_max_lai_cube_2d, title=f\"{target} max\")\n",
    "fig = cube_plotting(ref_std_lai_cube_2d, title=f\"{target} std\")\n",
    "fig = cube_plotting(\n",
    "    ref_std_lai_cube_2d / ref_avg_lai_cube_2d, title=f\"{target} std / mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Obs. FAPAR\"\n",
    "\n",
    "ref_fapar_cube_2d = regrid_to_n96e(\n",
    "    iris.load_cube(str(Path(DATA_DIR) / \"FAPAR_climatology.nc\"))\n",
    ")\n",
    "ref_fapar_cube_2d.data.mask |= match_shape(\n",
    "    ~get_n96e_land_mask(),\n",
    "    ref_fapar_cube_2d.shape,\n",
    ")\n",
    "\n",
    "display(HTML(ref_fapar_cube_2d._repr_html_()))\n",
    "\n",
    "ref_avg_fapar_cube_2d = ref_fapar_cube_2d.collapsed(\"time\", iris.analysis.MEAN)\n",
    "ref_max_fapar_cube_2d = ref_fapar_cube_2d.collapsed(\"time\", iris.analysis.MAX)\n",
    "ref_std_fapar_cube_2d = ref_avg_fapar_cube_2d.copy(\n",
    "    data=np.std(ref_fapar_cube_2d.data, axis=0)\n",
    ")\n",
    "\n",
    "fig = cube_plotting(ref_avg_fapar_cube_2d, title=f\"{target} mean\")\n",
    "fig = cube_plotting(ref_max_fapar_cube_2d, title=f\"{target} max\")\n",
    "fig = cube_plotting(ref_std_fapar_cube_2d, title=f\"{target} std\")\n",
    "fig = cube_plotting(\n",
    "    ref_std_fapar_cube_2d / ref_avg_fapar_cube_2d, title=f\"{target} std / mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = regions_GFED()\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_map = regions.attributes[\"regions\"]\n",
    "regions_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out the oceans.\n",
    "regions.data = np.ma.MaskedArray(regions.data, mask=regions.data == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regrid to N96e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n96e_regions = regrid_to_n96e(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n96e_regions.data.mask = n96e_regions.data == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply land mask.\n",
    "n96e_regions.data.mask |= ~get_n96e_land_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cube_plotting(regions, boundaries=np.arange(1, 16) - 0.5)\n",
    "fig = cube_plotting(n96e_regions, boundaries=np.arange(1, 16) - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Climatology comparison by region - with JULES data averaged over PFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the regions selection is working.\n",
    "# for data_cube in tqdm([pft_avg_cube_2d, ref_cube_2d]):\n",
    "#     for region in tqdm(range(1, 15)):  # Exclude the ocean.\n",
    "#         mask = match_shape(n96e_regions.data == region, data_cube.shape)\n",
    "#         plot_cube = data_cube.copy()\n",
    "#         plot_cube.data.mask |= ~mask\n",
    "#         cube_plotting(\n",
    "#             plot_cube, fig=plt.figure(figsize=(3, 1), dpi=100), title=str(region)\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the spatial averaging done here is not area weighted!\n",
    "for region in tqdm(range(1, 15)):  # Exclude the ocean.\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_title(regions_map[region])\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    handles = []\n",
    "\n",
    "    for label, data, ls in zip(\n",
    "        [\"JULES FAPAR\", \"OBS FAPAR\", \"JULES LAI\", \"OBS LAI\"],\n",
    "        [\n",
    "            get_climatology_cube(dummy_lat_lon_cube(fapar_2d)).data,\n",
    "            ref_fapar_cube_2d.data,\n",
    "            get_climatology_cube(dummy_lat_lon_cube(lai_2d)).data,\n",
    "            ref_lai_cube_2d.data,\n",
    "        ],\n",
    "        [\"--\", \"-\", \"--\", \"-\"],\n",
    "    ):\n",
    "        mask = match_shape(n96e_regions.data == region, data.shape)\n",
    "        plot_data = data.copy()\n",
    "        plot_data.mask |= ~mask\n",
    "        if \"FAPAR\" in label:\n",
    "            plot_ax = ax\n",
    "            plot_ax.set_ylabel(\"FAPAR\")\n",
    "            color = \"C0\"\n",
    "            alpha = 1.0\n",
    "            zorder = 2\n",
    "        elif \"LAI\" in label:\n",
    "            plot_ax = ax2\n",
    "            plot_ax.set_ylabel(\"LAI\")\n",
    "            color = \"C1\"\n",
    "            alpha = 0.7\n",
    "            zorder = 1\n",
    "\n",
    "        handles.append(\n",
    "            plot_ax.errorbar(\n",
    "                x=np.arange(1, plot_data.shape[0] + 1),\n",
    "                y=np.mean(plot_data, axis=(1, 2)),\n",
    "                yerr=np.std(plot_data, axis=(1, 2)),\n",
    "                capsize=4,\n",
    "                label=label,\n",
    "                linestyle=ls,\n",
    "                color=color,\n",
    "                alpha=alpha,\n",
    "                zorder=zorder,\n",
    "            )\n",
    "        )\n",
    "    ax.legend(handles=handles, ncol=1, bbox_to_anchor=(1.1, 1.12), loc=\"upper left\")\n",
    "    ax.set_xlabel(\"month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global comparison of FAPAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_mask = fapar_2d.mask | ref_fapar_cube_2d.data.mask\n",
    "plt.hexbin(\n",
    "    ref_fapar_cube_2d.data.data[~combined_mask],\n",
    "    fapar_2d.data[~combined_mask],\n",
    "    bins=\"log\",\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"OBS FAPAR\")\n",
    "plt.ylabel(\"JULES FAPAR\")\n",
    "xlim = plt.xlim()\n",
    "ylim = plt.ylim()\n",
    "plt.plot(np.linspace(0, 10, 100), np.linspace(0, 10, 100), ls=\"--\", c=\"C3\")\n",
    "plt.xlim(xlim)\n",
    "_ = plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global comparison of LAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_mask = lai_2d.mask | ref_lai_cube_2d.data.mask\n",
    "plt.hexbin(\n",
    "    ref_lai_cube_2d.data.data[~combined_mask], lai_2d.data[~combined_mask], bins=\"log\"\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"OBS LAI\")\n",
    "plt.ylabel(\"JULES LAI\")\n",
    "xlim = plt.xlim()\n",
    "ylim = plt.ylim()\n",
    "plt.plot(np.linspace(0, 10, 100), np.linspace(0, 10, 100), ls=\"--\", c=\"C3\")\n",
    "plt.xlim(xlim)\n",
    "_ = plt.ylim(ylim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wildfires] *",
   "language": "python",
   "name": "conda-env-wildfires-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
