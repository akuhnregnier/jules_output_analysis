{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import dask.array as da\n",
    "import iris\n",
    "from tqdm import tqdm\n",
    "from wildfires.data import homogenise_time_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = Path(\"/work/scratch-pw/alexkr/new-with-antec5\")\n",
    "data_dir = Path(\"~/tmp/new-with-antec6\").expanduser()\n",
    "files = list(data_dir.glob(\"*Instant.*.nc\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes = iris.cube.CubeList([])\n",
    "for f in tqdm(list(map(str, files))):\n",
    "    cubes.extend(iris.load_raw(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = len(cubes)\n",
    "print(len(cubes))\n",
    "cubes = iris.cube.CubeList([cube for cube in cubes if cube.shape[0] == 2190])\n",
    "print(len(cubes))\n",
    "if len(cubes) != orig_len:\n",
    "    print(\"warning - missing cubes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure cubes can be concatenated.\n",
    "concat = homogenise_time_coordinate(cubes).concatenate()\n",
    "# Ensure all cubes have the same number of temporal samples after concatenation.\n",
    "assert len(set(c.shape[0] for c in concat)) == 1\n",
    "print(len(concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cube in tqdm(concat):\n",
    "#     iris.coord_categorisation.add_day_of_year(cube, 'time')\n",
    "#     iris.coord_categorisation.add_hour(cube, 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# climatologies = iris.cube.CubeList()\n",
    "# for cube in tqdm(concat):\n",
    "#     climatologies.append(cube.aggregated_by([\"day_of_year\", \"hour\"], iris.analysis.MEAN))\n",
    "# climatologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proper 'Dask-way' with lazy mean for each variable - very slow on jasmin sci\n",
    "# servers (expected ~9 hrs for 47 variables across 17 years with 4 hour resolution).\n",
    "climatologies = iris.cube.CubeList()\n",
    "n_matching = set()\n",
    "for concat_cube in tqdm(concat):\n",
    "    matching = cubes.extract(iris.Constraint(name=concat_cube.name()))\n",
    "    n_matching.add(len(matching))\n",
    "    # NOTE - the time coordinate will technically be wrong here for the climatological case.\n",
    "    climatologies.append(\n",
    "        matching[0].copy(\n",
    "            data=da.mean(\n",
    "                da.stack(tuple(cube.core_data() for cube in matching), axis=0),\n",
    "                axis=0\n",
    "                # If computing here already, the time estimate will be accurate, but all final cubes will have to be held in memory at once!\n",
    "            )  # .compute()\n",
    "        )\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "assert len(n_matching) == 1, n_matching\n",
    "\n",
    "climatologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proper 'numpy-way' - realise data one variable at a time and take the mean.\n",
    "# Uses more memory than the above.\n",
    "# climatologies = iris.cube.CubeList()\n",
    "# n_matching = set()\n",
    "# for concat_cube in tqdm(concat):\n",
    "#     matching = cubes.extract(iris.Constraint(name=concat_cube.name()))\n",
    "#     n_matching.add(len(matching))\n",
    "#     # NOTE - the time coordinate will technically be wrong here for the climatological case.\n",
    "#     climatologies.append(\n",
    "#         matching[0].copy(\n",
    "#             data=np.mean(\n",
    "#                 np.stack(\n",
    "#                     tuple(cube.data for cube in matching),\n",
    "#                     axis=0\n",
    "#                 ),\n",
    "#                 axis=0\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "#     gc.collect()\n",
    "\n",
    "# assert len(n_matching) == 1, n_matching\n",
    "\n",
    "# climatologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use numpy, but try to minimise memory usage by incrementally adding individual arrays before dividing.\n",
    "# climatologies = iris.cube.CubeList()\n",
    "# n_matching = set()\n",
    "# for concat_cube in tqdm(concat):\n",
    "#     matching = cubes.extract(iris.Constraint(name=concat_cube.name()))\n",
    "#     n_matching.add(len(matching))\n",
    "#     # NOTE - the time coordinate will technically be wrong here for the climatological case.\n",
    "#     climatology_data = matching[0].data\n",
    "#     for cube in matching[1:]:\n",
    "#         climatology_data += cube.data\n",
    "#         gc.collect()\n",
    "#     climatology_data /= len(matching)\n",
    "\n",
    "#     climatologies.append(matching[0].copy(data=climatology_data))\n",
    "\n",
    "#     gc.collect()\n",
    "\n",
    "# assert len(n_matching) == 1, n_matching\n",
    "\n",
    "# climatologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.save(climatologies, str(Path(\"~/tmp/climatology6.nc\").expanduser()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wildfires] *",
   "language": "python",
   "name": "conda-env-wildfires-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
