{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from alepython import ale_plot\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from utils import (\n",
    "    collapse_cube_dim,\n",
    "    cube_1d_to_2d,\n",
    "    get_mm_data,\n",
    "    param_dict,\n",
    "    train_test_split_kwargs,\n",
    ")\n",
    "from wildfires.analysis import cube_plotting\n",
    "from wildfires.data import MonthlyDataset, homogenise_time_coordinate\n",
    "from wildfires.utils import ensure_datetime\n",
    "\n",
    "filterwarnings(\"ignore\", \".*divide by zero.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = Path(\"~/JULES_output/jules_output5\").expanduser()\n",
    "assert source_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes = homogenise_time_coordinate(\n",
    "    iris.load(str(source_dir / \"*Monthly*.nc\"))\n",
    ").concatenate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cube in tqdm(cubes, desc=\"Plotting cubes (single timeslice)\"):\n",
    "    cube_2d = cube_1d_to_2d(cube[0])\n",
    "\n",
    "    assert len(cube_2d.shape) >= 2\n",
    "\n",
    "    if len(cube_2d.shape) == 2:\n",
    "        sel = slice(None)\n",
    "    else:\n",
    "        for indices in product(*(range(l) for l in cube_2d.shape[:-2])):\n",
    "            sel = (*indices, slice(None), slice(None))\n",
    "    try:\n",
    "        fig = cube_plotting(cube_2d[sel])\n",
    "    except Exception as e:\n",
    "        print(\"cube:\", str(cube))\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_cube = cubes.extract_strict(iris.Constraint(name=\"Gridbox mean burnt area fraction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_2d = cube_1d_to_2d(ba_cube[10][0])\n",
    "ba_2d.data.mask |= np.isnan(ba_2d.data)\n",
    "fig = cube_plotting(ba_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted({cube.long_name for cube in cubes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_names = [\n",
    "    \"Gridbox precipitation rate\",\n",
    "    \"Gridbox soil carbon (total)\",\n",
    "    \"Gridbox soil carbon in each pool (DPM,RPM,bio,hum)\",\n",
    "    \"Gridbox surface evapotranspiration from soil moisture store\",\n",
    "    # \"Gridbox surface temperature\",\n",
    "    \"Gridbox effective radiative temperature (assuming emissivity=1)\",\n",
    "    # \"Gridbox unfrozen moisture content of each soil layer as a fraction of saturation\",\n",
    "    \"Gridbox unfrozen soil moisture as fraction of saturation\",\n",
    "    \"PFT burnt area fraction\",\n",
    "    \"PFT gross primary productivity\",\n",
    "    \"PFT leaf area index\",\n",
    "    # \"PFT net primary productivity\",\n",
    "    \"NPP (GBM) post N-limitation\",\n",
    "    \"PFT net primary productivity prior to N limitation\",\n",
    "    # \"PFT total carbon content of the vegetation at the end of model timestep.\",\n",
    "    \"C in decomposable plant material\",\n",
    "    \"C in decomposable plant material, gridbox total\",\n",
    "    \"C in humus\",\n",
    "    \"C in resistant plant material\",\n",
    "    \"C in resistant plant material, gridbox total\",\n",
    "    \"C in soil biomass\",\n",
    "    \"C in soil biomass, gridbox total\",\n",
    "    \"C in soil humus, gridbox total\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_dict = {\n",
    "    \"UK\": (51.5, 0),\n",
    "    \"Uganda\": (2.36, 32.51),\n",
    "    \"Durban\": (-29.13, 31),\n",
    "    \"Cape Town\": (-33.57, 19.28),\n",
    "    \"Yosemite\": (37.7, 360 - 119.67),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_months = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_carbon_pools = (\"DPM\", \"RPM\", \"bio\", \"hum\")\n",
    "pfts = list(range(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with individual PFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (location, (lat, lon)) in lat_lon_dict.items():\n",
    "    fig, axes = plt.subplots(\n",
    "        len(cube_names),\n",
    "        1,\n",
    "        figsize=(10, 3.5 * len(cube_names)),\n",
    "        sharex=True,\n",
    "        constrained_layout=True,\n",
    "        dpi=120,\n",
    "    )\n",
    "    fig.suptitle(location)\n",
    "\n",
    "    for ax, var in zip(axes, cube_names):\n",
    "        ext_cube = cubes.extract_strict(iris.Constraint(name=var))\n",
    "        var_cube = cube_1d_to_2d(ext_cube)[-n_months:]\n",
    "\n",
    "        print(var_cube.long_name, var_cube.shape)\n",
    "\n",
    "        ax.set_title(f\"{var_cube.long_name} ({var_cube.units})\")\n",
    "\n",
    "        lat_i = np.argmin(np.abs(var_cube.coord(\"latitude\").points - lat))\n",
    "        lat_sel = var_cube.coord(\"latitude\").points[lat_i]\n",
    "\n",
    "        lon_i = np.argmin(np.abs(var_cube.coord(\"longitude\").points - lon))\n",
    "        lon_sel = var_cube.coord(\"longitude\").points[lon_i]\n",
    "\n",
    "        if len(var_cube.shape) == 3:\n",
    "            plot_data_list = [var_cube[:, lat_i, lon_i].data.copy()]\n",
    "        elif len(var_cube.shape) == 4:\n",
    "            plot_data_list = [\n",
    "                var_cube[:, i, lat_i, lon_i].data.copy()\n",
    "                for i in range(var_cube.shape[1])\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        labels = list(map(str, range(len(plot_data_list))))\n",
    "        if len(var_cube.shape) == 4:\n",
    "            if var_cube.shape[1] == 4:\n",
    "                labels = soil_carbon_pools\n",
    "            elif var_cube.shape[1] == 5:\n",
    "                labels = [\"BT\", \"NT\", \"C3\", \"C4\", \"SH\"]\n",
    "\n",
    "        for plot_data, label in zip(plot_data_list, labels):\n",
    "            # plot_data_mean = np.mean(plot_data)\n",
    "            # plot_data -= plot_data_mean\n",
    "\n",
    "            ax.plot(\n",
    "                [\n",
    "                    ensure_datetime(var_cube.coord(\"time\").cell(i).point)\n",
    "                    for i in range(var_cube.shape[0])\n",
    "                ],\n",
    "                plot_data,\n",
    "                label=label,\n",
    "                marker=\".\",\n",
    "            )\n",
    "        ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with mean over individual PFTs and pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(\n",
    "#     len(cube_names),\n",
    "#     1,\n",
    "#     figsize=(9, 3 * len(cube_names)),\n",
    "#     sharex=True,\n",
    "#     constrained_layout=True,\n",
    "#     dpi=120,\n",
    "# )\n",
    "\n",
    "# for ax, var in zip(axes, cube_names):\n",
    "#     ext_cube = cubes.extract_strict(iris.Constraint(name=var))\n",
    "#     if \"generic\" in list(coord.name() for coord in ext_cube.coords()):\n",
    "#         # Average over carbon pools or PFTs\n",
    "#         ext_cube = ext_cube.collapsed(\"generic\", iris.analysis.MEAN)\n",
    "\n",
    "#     var_cube = cube_1d_to_2d(ext_cube)[-n_months:]\n",
    "\n",
    "#     print(var_cube.long_name, var_cube.shape)\n",
    "\n",
    "#     ax.set_title(f\"{var_cube.long_name} ({var_cube.units})\")\n",
    "\n",
    "#     for (location, (lat, lon)) in lat_lon_dict.items():\n",
    "#         lat_i = np.argmin(np.abs(var_cube.coord(\"latitude\").points - lat))\n",
    "#         lat_sel = var_cube.coord(\"latitude\").points[lat_i]\n",
    "\n",
    "#         lon_i = np.argmin(np.abs(var_cube.coord(\"longitude\").points - lon))\n",
    "#         lon_sel = var_cube.coord(\"longitude\").points[lon_i]\n",
    "\n",
    "#         assert len(var_cube.shape) == 3\n",
    "\n",
    "#         plot_data = var_cube[:, lat_i, lon_i].data.copy()\n",
    "\n",
    "#         # plot_data_mean = np.mean(plot_data)\n",
    "#         # plot_data -= plot_data_mean\n",
    "\n",
    "#         ax.plot(\n",
    "#             [\n",
    "#                 ensure_datetime(var_cube.coord(\"time\").cell(i).point)\n",
    "#                 for i in range(var_cube.shape[0])\n",
    "#             ],\n",
    "#             plot_data,\n",
    "#             label=location,\n",
    "#             marker=\".\",\n",
    "#         )\n",
    "#     ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_months = 150\n",
    "\n",
    "processed_cubes = iris.cube.CubeList([])\n",
    "for var in tqdm(cube_names, desc=\"Processing cubes\"):\n",
    "    ext_cube = cubes.extract_strict(iris.Constraint(name=var))\n",
    "\n",
    "    if len(ext_cube.shape) > 3:\n",
    "        ext_cube = collapse_cube_dim(ext_cube, 1)\n",
    "\n",
    "    # If needed, apply the same operation once again.\n",
    "    if len(ext_cube.shape) == 4:\n",
    "        ext_cube = collapse_cube_dim(ext_cube, 1)\n",
    "\n",
    "    assert len(ext_cube.shape) == 3\n",
    "\n",
    "    var_cube = cube_1d_to_2d(ext_cube)[-proc_months:]\n",
    "    processed_cubes.append(var_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'proper' averaging - requires more RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Raw, shifted datasets.\n",
    "# raw_proc_insts = []\n",
    "\n",
    "# for proc_cube in tqdm(processed_cubes):\n",
    "#     # Create a new Dataset for each cube.\n",
    "#     proc_inst = type(\n",
    "#         proc_cube.long_name.replace(\" \", \"\"),\n",
    "#         (MonthlyDataset,),\n",
    "#         {\n",
    "#             \"__init__\": lambda self: None,\n",
    "#             \"frequency\": \"monthly\",\n",
    "#         },\n",
    "#     )()\n",
    "#     proc_inst.cubes = iris.cube.CubeList([proc_cube])\n",
    "\n",
    "#     raw_proc_insts.append(proc_inst)\n",
    "\n",
    "#     if \"burnt area\" not in proc_inst.cube.long_name:\n",
    "#         # Shift if applicable, i.e. not BA.\n",
    "#         for shift in (1, 3, 6, 9):\n",
    "#             raw_proc_insts.append(\n",
    "#                 proc_inst.get_temporally_shifted_dataset(months=-shift, deep=False)\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clim_proc_insts = prepare_selection(Datasets(raw_proc_insts), which=\"climatology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_proc_insts.homogenise_masks()\n",
    "# raw_proc_insts.apply_masks(ba_cube.data.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_insts = []\n",
    "\n",
    "for proc_cube in tqdm(processed_cubes):\n",
    "    # Create a new Dataset for each cube.\n",
    "    proc_inst = type(\n",
    "        proc_cube.long_name.replace(\" \", \"\"),\n",
    "        (MonthlyDataset,),\n",
    "        {\n",
    "            \"__init__\": lambda self: None,\n",
    "            \"frequency\": \"monthly\",\n",
    "        },\n",
    "    )()\n",
    "    proc_inst.cubes = iris.cube.CubeList([proc_cube])\n",
    "\n",
    "    proc_insts.append(\n",
    "        proc_inst.get_climatology_dataset(proc_inst.min_time, proc_inst.max_time)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_proc_cubes = []\n",
    "\n",
    "ba_cube = None\n",
    "\n",
    "for proc_inst in proc_insts:\n",
    "    if \"burnt area\" in proc_inst.cube.long_name:\n",
    "        ba_cube = proc_inst.cube\n",
    "        continue\n",
    "    shifted_proc_cubes.append(proc_inst.cube)\n",
    "    for shift in (1, 3, 6, 9):\n",
    "        # XXX: Overly simplistic np.roll() implementation!!\n",
    "        c2 = proc_inst.cube.copy()\n",
    "        c2.data = np.roll(proc_inst.cube.data, shift, axis=0)\n",
    "        c2.long_name = c2.long_name + f\"({shift})\"\n",
    "        c2.var_name = None\n",
    "        c2.short_name = None\n",
    "\n",
    "        shifted_proc_cubes.append(c2)\n",
    "\n",
    "assert ba_cube is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cube.coords(\"month_number\"):\n",
    "    iris.coord_categorisation.add_month_number(cube, \"time\")\n",
    "\n",
    "plt.figure(figsize=(5, 3), dpi=150)\n",
    "for i in np.array([0, 1, 2]) + 35:\n",
    "    print(cube.coord(\"month_number\").points)\n",
    "    cube = shifted_proc_cubes[i]\n",
    "    plt.plot(\n",
    "        cube.coord(\"month_number\").points,\n",
    "        cube.data[:, 72, 110],\n",
    "        label=cube.name(),\n",
    "        alpha=0.9,\n",
    "        marker=\"x\",\n",
    "    )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_mask = ba_cube.data.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog_data = pd.Series(ba_cube.data.data[~master_mask])\n",
    "endog_data.name = \"burnt area\"\n",
    "\n",
    "exog_dict = {}\n",
    "for cube in shifted_proc_cubes:\n",
    "    exog_dict[cube.long_name] = cube.data.data[~master_mask]\n",
    "\n",
    "exog_data = pd.DataFrame(exog_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten_mapping = {\n",
    "    \"Gridbox\": \"\",\n",
    "    \"precipitation\": \"precip\",\n",
    "    \"soil carbon in each pool\": \"soil pool carbon\",\n",
    "    \"surface\": \"surf\",\n",
    "    \"evapotranspiration\": \"evapot\",\n",
    "    \"from soil moisture store\": \"soil moist\",\n",
    "    \"temperature\": \"temp\",\n",
    "    \"unfrozen moisture content of each soil layer as a fraction of saturation\": \"unfrozen moist soil layer / sat\",\n",
    "    \"gross primary productivity\": \"gpp\",\n",
    "    \"net primary productivity\": \"npp\",\n",
    "    \"soil moisture availability factor (beta)\": \"soil moist avail fact\",\n",
    "    \"total carbon content of the vegetation at the end of model timestep\": \"total veg C end timestep\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_columns(df):\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        for old, new in shorten_mapping.items():\n",
    "            col = col.replace(old, new)\n",
    "        col = col.strip()\n",
    "        new_cols.append(col)\n",
    "    df.columns = new_cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten_columns(exog_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the BA data so it has higher magnitudes\n",
    "\n",
    "This seems to be required by the RF algorithm in order to make any predictions at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog_data /= np.max(endog_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    exog_data, endog_data, **train_test_split_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(**param_dict)\n",
    "model.n_jobs = 3\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train:\", r2_score(y_train_pred, y_train))\n",
    "print(\"val:\", r2_score(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test.values)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cube_plotting(\n",
    "    get_mm_data(y_test.values, master_mask, \"val\"),\n",
    "    boundaries=[0, 1e-3, 1e-2, 0.05, 0.2, 0.5],\n",
    "    cmap=\"inferno\",\n",
    "    fig=plt.figure(figsize=(7, 3.2), dpi=150),\n",
    "    colorbar_kwargs=dict(label=\"burnt area (scaled)\"),\n",
    "    title=\"Validation Observations\",\n",
    "    extend=\"neither\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cube_plotting(\n",
    "    get_mm_data(y_test_pred, master_mask, \"val\"),\n",
    "    boundaries=[0, 1e-3, 1e-2, 0.05, 0.2, 0.5],\n",
    "    cmap=\"inferno\",\n",
    "    fig=plt.figure(figsize=(7, 3.2), dpi=150),\n",
    "    colorbar_kwargs=dict(label=\"burnt area (scaled)\"),\n",
    "    title=\"Validation Predictions\",\n",
    "    extend=\"neither\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250)\n",
    "plt.hexbin(y_train, y_train_pred, bins=\"log\")\n",
    "plt.xlabel(\"observed BA (train)\")\n",
    "_ = plt.ylabel(\"predicted BA (train)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250)\n",
    "plt.hexbin(y_test, y_test_pred, bins=\"log\")\n",
    "plt.xlabel(\"observed BA (test)\")\n",
    "_ = plt.ylabel(\"predicted BA (test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_trees_gini = pd.DataFrame(\n",
    "    [tree.feature_importances_ for tree in model],\n",
    "    columns=X_train.columns,\n",
    ")\n",
    "mean_importances = ind_trees_gini.mean().sort_values(ascending=False)\n",
    "ind_trees_gini = ind_trees_gini.reindex(mean_importances.index, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4), dpi=170)\n",
    "\n",
    "N_col = 18\n",
    "\n",
    "sns.boxplot(data=ind_trees_gini.iloc[:, :N_col], ax=ax)\n",
    "ax.set(\n",
    "    # title=\"Gini Importances\",\n",
    "    ylabel=\"Gini Importance\\n\"\n",
    ")\n",
    "_ = plt.setp(ax.xaxis.get_majorticklabels(), rotation=60, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_trees_gini = pd.DataFrame(\n",
    "    [tree.feature_importances_ for tree in model],\n",
    "    columns=X_train.columns,\n",
    ")\n",
    "mean_importances = ind_trees_gini.mean().sort_values(ascending=False)\n",
    "ind_trees_gini = ind_trees_gini.reindex(mean_importances.index, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4), dpi=170)\n",
    "\n",
    "N_col = 30\n",
    "\n",
    "sns.boxplot(data=ind_trees_gini.iloc[:, :N_col], ax=ax)\n",
    "ax.set(\n",
    "    # title=\"Gini Importances\",\n",
    "    ylabel=\"Gini Importance\\n\"\n",
    ")\n",
    "_ = plt.setp(ax.xaxis.get_majorticklabels(), rotation=60, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in tqdm(X_train.columns, desc=\"1D ALE plots\"):\n",
    "    fig, axes = ale_plot(\n",
    "        model,\n",
    "        X_train,\n",
    "        feature,\n",
    "        bins=20,\n",
    "        fig=plt.figure(figsize=(5.5, 2), dpi=150),\n",
    "        quantile_axis=True,\n",
    "        monte_carlo=True,\n",
    "        monte_carlo_rep=10,\n",
    "        monte_carlo_ratio=0.2,\n",
    "    )\n",
    "    plt.setp(axes[\"ale\"].xaxis.get_majorticklabels(), rotation=60, ha=\"right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wildfires] *",
   "language": "python",
   "name": "conda-env-wildfires-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
